
# ğŸŒŸ Image Description Generator

A Python-based tool for generating captions for images using the **BLIP (Bootstrapped Language-Image Pretraining)** model from Salesforce.

## âœ¨ Features
- ğŸ“‚ Automatically generates captions for all images in the `images` folder.
- âš¡ Supports processing multiple images in bulk.
- ğŸ“„ Saves the generated captions in a CSV file in the `output` folder.
- ğŸ› ï¸ Modular design with reusable scripts for core functionality.

## ğŸ¤– The Model: BLIP (Bootstrapped Language-Image Pretraining)
This project leverages the **BLIP (Base)** model, a pretrained vision-language model developed by Salesforce. BLIP is designed for tasks like image captioning, visual question answering, and image-text retrieval.

### ğŸ” Why BLIP?
BLIP provides:
- **ğŸš€ State-of-the-art performance**: Excellent results in generating natural language descriptions for images.
- **ğŸŒ Pretrained on diverse datasets**: The model understands a wide range of concepts and objects.
- **ğŸ”§ Flexibility**: Works well out-of-the-box for image captioning without requiring fine-tuning.

### Model Details
- **Model Name**: `Salesforce/blip-image-captioning-base`
- **Architecture**: Vision Transformer (ViT) combined with a Transformer-based language model.
- **Source**: [Hugging Face Transformers](https://huggingface.co/Salesforce/blip-image-captioning-base)

For more information about BLIP, visit the [official paper](https://arxiv.org/abs/2201.12086).

## ğŸ› ï¸ Installation

1. Clone the repository:
    ```bash
    git clone https://github.com/your-repo/image-description-generator.git
    cd image-description-generator
    ```

2. Install the dependencies:
    ```bash
    pip install -r requirements.txt
    ```

## ğŸš€ Usage

1. ğŸ“‚ Place images in the `images` folder (e.g., `images/horse.jpg`).
2. â–¶ï¸ Run the main script:
    ```bash
    python main.py
    ```
3. âœ… Check the generated captions in the `output/captions.csv`.

### âš™ï¸ Command-Line Arguments
You can customize the behavior of the program by modifying the parameters in the scripts or adding command-line arguments. Current configuration includes:
- ğŸ“ Input folder: `images/`
- ğŸ“„ Output file: `output/captions.csv`
- âœï¸ Number of words: Default is 10 words per caption.

## ğŸ“‚ Folder Structure

The repository is organized as follows:

```
image-description-generator/
â”œâ”€â”€ images/                 # Folder for input images
â”‚   â”œâ”€â”€ horse.jpg
â”‚   â”œâ”€â”€ beach.jpg
â”‚   â””â”€â”€ ... (other images)
â”œâ”€â”€ output/                 # Folder for generated outputs
â”‚   â””â”€â”€ captions.csv        # Captions generated for images
â”œâ”€â”€ scripts/                # Python scripts for core functionality
â”‚   â”œâ”€â”€ describe.py         # Logic for generating captions
â”‚   â””â”€â”€ utils.py            # Helper functions
â”œâ”€â”€ main.py                 # Main script to execute the program
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ README.md               # Documentation
â””â”€â”€ LICENSE                 # License file
```

## ğŸ› ï¸ How It Works

1. **ğŸ–¼ï¸ Image Preprocessing**:
    - Each image in the `images/` folder is processed and converted into a format compatible with the BLIP model using the Hugging Face `transformers` library.
    
2. **âœï¸ Caption Generation**:
    - The BLIP model generates captions based on the visual features of the image.
    - Captions are generated with a configurable word limit (default: 10 words).

3. **ğŸ“„ Output**:
    - Captions are saved in `output/captions.csv` with the format:
      ```
      Image,Caption
      horse.jpg,A brown horse galloping through a green field.
      beach.jpg,A beautiful sunset over a sandy beach with waves.
      ```

## ğŸ¤ Contributing

Contributions are welcome! Open an issue or submit a pull request for improvements.

## ğŸ“œ License

This project is licensed under the MIT License.
